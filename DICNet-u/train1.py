import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
from Net.DICNet_corr import DICNet  # å¯¼å…¥è‡ªå®šä¹‰DICNetç½‘ç»œ
from dataset1.dataset1 import DICDataset  # å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†ç±»
from torch.optim.lr_scheduler import StepLR
from util import AverageMeter  # å‡è®¾ç”¨äºç»Ÿè®¡æŒ‡æ ‡çš„å·¥å…·ç±»


# -------------------------- 1. æ ¸å¿ƒæŸå¤±å‡½æ•°ä¸è¯„ä¼°æŒ‡æ ‡ --------------------------
def pearson_corr_loss(y_true, y_pred):
    """çš®å°”é€Šç›¸å…³ç³»æ•°æŸå¤±ï¼Œæ·»åŠ 1e-8é˜²æ­¢é™¤é›¶"""
    x = y_true.flatten()
    y = y_pred.flatten()
    vx = x - torch.mean(x)
    vy = y - torch.mean(y)
    rho = torch.sum(vx * vy) / (
            torch.sqrt(torch.sum(vx ** 2) + 1e-8) * torch.sqrt(torch.sum(vy ** 2) + 1e-8)
    )
    return 1 - rho


def calculate_aee(pred_disp, gt_disp):
    """è®¡ç®—å¹³å‡ç«¯ç‚¹è¯¯å·®(Average Endpoint Error)"""
    return torch.mean(torch.norm(pred_disp - gt_disp, p=2, dim=1))  # æŒ‰é€šé“è®¡ç®—L2èŒƒæ•°ï¼Œå†å–å¹³å‡


# -------------------------- 2. æ¨¡å‹ä¿å­˜/æ¢å¤å·¥å…· --------------------------
def save_model(model, epoch, optimizer, scheduler, save_dir, model_name, is_best=False):
    os.makedirs(save_dir, exist_ok=True)
    if is_best:
        save_path = os.path.join(save_dir, "best_" + model_name)
    else:
        save_path = os.path.join(save_dir, f"epoch_{epoch}_" + model_name)

    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "scheduler_state_dict": scheduler.state_dict() if scheduler else None,
    }, save_path)
    print(f"âœ… Model saved to {save_path}")


def load_model(model, optimizer, scheduler, load_path):
    best_val_loss = float("inf")
    start_epoch = 0

    if os.path.exists(load_path):
        checkpoint = torch.load(load_path, map_location=next(model.parameters()).device)
        model.load_state_dict(checkpoint["model_state_dict"])
        if optimizer and "optimizer_state_dict" in checkpoint:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        if scheduler and "scheduler_state_dict" in checkpoint and checkpoint["scheduler_state_dict"]:
            scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        start_epoch = checkpoint["epoch"] + 1
        print(f"ğŸ”„ Loaded model from {load_path}, start at epoch {start_epoch}")
    else:
        print(f"âš ï¸ Model path {load_path} not found, training from scratch")

    return start_epoch, best_val_loss


# -------------------------- 3. å›¾åƒ warp å‡½æ•° --------------------------
def get_predicted_reference_image(def_img, displacements):
    batch_size, _, height, width = displacements.shape
    grid_y, grid_x = torch.meshgrid(
        torch.linspace(-1, 1, height, device=def_img.device),
        torch.linspace(-1, 1, width, device=def_img.device),
        indexing="ij"
    )
    grid = torch.stack((grid_x, grid_y), dim=2).unsqueeze(0).repeat(batch_size, 1, 1, 1)

    flow = displacements.permute(0, 2, 3, 1)
    flow[..., 0] /= (width / 2)
    flow[..., 1] /= (height / 2)

    warped_grid = grid - flow
    predicted_ref_img = F.grid_sample(
        def_img,
        warped_grid,
        mode="bilinear",
        padding_mode="border",
        align_corners=True
    )
    return predicted_ref_img


# -------------------------- 4. éªŒè¯æµç¨‹ --------------------------
def validate_supervised(model, val_loader, criterion, device, val_writer, epoch):
    model.eval()
    val_loss_meter = AverageMeter()
    val_aee_meter = AverageMeter()

    with torch.no_grad():
        for ref_img, def_img, gt_disp_x, gt_disp_y in val_loader:
            ref_img = ref_img.float().to(device)
            def_img = def_img.float().to(device)
            gt_disp = torch.cat((gt_disp_x, gt_disp_y), dim=1).float().to(device)

            pred_disp = model(torch.cat((ref_img, def_img), dim=1))
            loss = criterion(pred_disp, gt_disp)
            aee = calculate_aee(pred_disp, gt_disp)

            val_loss_meter.update(loss.item(), ref_img.size(0))
            val_aee_meter.update(aee.item(), ref_img.size(0))

    # è®°å½•éªŒè¯é›†æŒ‡æ ‡
    val_writer.add_scalar("Loss", val_loss_meter.avg, epoch)
    val_writer.add_scalar("AEE", val_aee_meter.avg, epoch)

    model.train()
    return val_loss_meter.avg, val_aee_meter.avg


def validate_unsupervised(model, val_loader, device, val_writer, epoch):
    model.eval()
    val_loss_meter = AverageMeter()
    val_aee_meter = AverageMeter()
    mse_criterion = nn.MSELoss()

    with torch.no_grad():
        for ref_img, def_img, gt_disp_x, gt_disp_y in val_loader:  # æ— ç›‘ç£éªŒè¯ä»éœ€çœŸå®ä½ç§»è®¡ç®—AEE
            ref_img = ref_img.float().to(device)
            def_img = def_img.float().to(device)
            gt_disp = torch.cat((gt_disp_x, gt_disp_y), dim=1).float().to(device)

            pred_disp = model(torch.cat((ref_img, def_img), dim=1))
            predicted_ref_img = get_predicted_reference_image(def_img, pred_disp)

            mse_loss = mse_criterion(predicted_ref_img, ref_img)
            corr_loss = pearson_corr_loss(predicted_ref_img, ref_img)
            total_loss = mse_loss + corr_loss
            aee = calculate_aee(pred_disp, gt_disp)

            val_loss_meter.update(total_loss.item(), ref_img.size(0))
            val_aee_meter.update(aee.item(), ref_img.size(0))

    # è®°å½•éªŒè¯é›†æŒ‡æ ‡
    val_writer.add_scalar("Loss", val_loss_meter.avg, epoch)
    val_writer.add_scalar("AEE", val_aee_meter.avg, epoch)

    model.train()
    return val_loss_meter.avg, val_aee_meter.avg


# -------------------------- 5. è®­ç»ƒæµç¨‹ --------------------------
def pretrain_supervised(model, train_loader, val_loader, optimizer, scheduler, criterion,
                        device, save_dir, start_epoch=0, total_epochs=30):
    best_val_loss = float("inf")
    # åˆå§‹åŒ–è®­ç»ƒå’ŒéªŒè¯æ—¥å¿—å†™å…¥å™¨
    train_writer = SummaryWriter(os.path.join(save_dir, "pretrain_log1"))
    val_writer = SummaryWriter(os.path.join(save_dir, "pretrain_val_log1"))

    for epoch in range(start_epoch, total_epochs):
        model.train()
        train_loss_meter = AverageMeter()
        train_aee_meter = AverageMeter()

        # è®­ç»ƒè¿­ä»£
        for batch_idx, (ref_img, def_img, gt_disp_x, gt_disp_y) in enumerate(train_loader):
            ref_img = ref_img.float().to(device)
            def_img = def_img.float().to(device)
            gt_disp = torch.cat((gt_disp_x, gt_disp_y), dim=1).float().to(device)

            optimizer.zero_grad()
            pred_disp = model(torch.cat((ref_img, def_img), dim=1))
            loss = criterion(pred_disp, gt_disp)
            aee = calculate_aee(pred_disp, gt_disp)

            loss.backward()
            optimizer.step()

            train_loss_meter.update(loss.item(), ref_img.size(0))
            train_aee_meter.update(aee.item(), ref_img.size(0))

            print(f"ğŸ“Œ Pretrain Epoch[{epoch + 1}/{total_epochs}] Batch[{batch_idx + 1}/{len(train_loader)}] "
                  f"Loss: {loss.item():.6f}, AEE: {aee.item():.6f}")

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # è®°å½•è®­ç»ƒé›†æŒ‡æ ‡
        train_writer.add_scalar("Loss", train_loss_meter.avg, epoch)
        train_writer.add_scalar("AEE", train_aee_meter.avg, epoch)
        train_writer.add_scalar("LearningRate", optimizer.param_groups[0]['lr'], epoch)

        # éªŒè¯è¿­ä»£
        val_loss, val_aee = validate_supervised(model, val_loader, criterion, device, val_writer, epoch)

        # æ‰“å° epoch æ€»ç»“
        print(f"ğŸ“Š Pretrain Epoch[{epoch + 1}/{total_epochs}] "
              f"Train Loss: {train_loss_meter.avg:.6f}, Train AEE: {train_aee_meter.avg:.6f} | "
              f"Val Loss: {val_loss:.6f}, Val AEE: {val_aee:.6f}")

        # é¢„è®­ç»ƒé˜¶æ®µæ¨¡å‹ä¿å­˜é€»è¾‘ï¼ˆä¿®æ”¹åï¼‰
        # 1. ä¿å­˜å½“å‰epochçš„æ¨¡å‹ï¼ˆæ¯ä¸ªepochéƒ½ä¿å­˜ï¼Œå¸¦epochç¼–å·ï¼‰
        save_model(
            model=model,
            epoch=epoch,
            optimizer=optimizer,
            scheduler=scheduler,
            save_dir=os.path.join(save_dir, "model1"),
            model_name=f"pretrain_dicnet1_epoch_{epoch}.pth",  # æ¯ä¸ªepochå•ç‹¬å‘½å
            is_best=False  # éæœ€ä½³æ¨¡å‹æ ‡è®°
        )

        # 2. è‹¥å½“å‰éªŒè¯æŸå¤±æ›´ä¼˜ï¼Œä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹ï¼ˆå›ºå®šåç§°ï¼‰
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_model(
                model=model,
                epoch=epoch,
                optimizer=optimizer,
                scheduler=scheduler,
                save_dir=os.path.join(save_dir, "model1"),
                model_name="pretrain_dicnet1_best.pth",  # å›ºå®šæœ€ä½³æ¨¡å‹åç§°
                is_best=True  # æœ€ä½³æ¨¡å‹æ ‡è®°
            )
            print(f"ğŸŒŸ New best val loss: {best_val_loss:.6f} (saved as best model)")

    # å…³é—­å†™å…¥å™¨
    train_writer.close()
    val_writer.close()
    print("âœ… Pretrain completed!")
    return best_val_loss


def train_unsupervised(model, train_loader, val_loader, optimizer, scheduler, device,
                       save_dir, start_epoch=0, total_epochs=270):
    best_val_loss = float("inf")
    # åˆå§‹åŒ–è®­ç»ƒå’ŒéªŒè¯æ—¥å¿—å†™å…¥å™¨
    train_writer = SummaryWriter(os.path.join(save_dir, "train_log1"))
    val_writer = SummaryWriter(os.path.join(save_dir, "val_log1"))
    mse_criterion = nn.MSELoss()

    for epoch in range(start_epoch, total_epochs):
        model.train()
        train_loss_meter = AverageMeter()
        train_mse_meter = AverageMeter()
        train_corr_meter = AverageMeter()
        train_aee_meter = AverageMeter()  # è®­ç»ƒé›†ä¹Ÿè®¡ç®—AEEï¼ˆéœ€è¦æ ‡æ³¨ï¼‰

        # è®­ç»ƒè¿­ä»£
        for batch_idx, (ref_img, def_img, gt_disp_x, gt_disp_y) in enumerate(train_loader):
            ref_img = ref_img.float().to(device)
            def_img = def_img.float().to(device)
            gt_disp = torch.cat((gt_disp_x, gt_disp_y), dim=1).float().to(device)

            optimizer.zero_grad()
            pred_disp = model(torch.cat((ref_img, def_img), dim=1))
            predicted_ref_img = get_predicted_reference_image(def_img, pred_disp)

            # è®¡ç®—æŸå¤±
            mse_loss = mse_criterion(predicted_ref_img, ref_img)
            corr_loss = pearson_corr_loss(predicted_ref_img, ref_img)
            total_loss = mse_loss + corr_loss
            # è®¡ç®—AEEï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼‰
            aee = calculate_aee(pred_disp, gt_disp)

            total_loss.backward()
            optimizer.step()

            # æ›´æ–°ç»Ÿè®¡
            train_loss_meter.update(total_loss.item(), ref_img.size(0))
            train_mse_meter.update(mse_loss.item(), ref_img.size(0))
            train_corr_meter.update(corr_loss.item(), ref_img.size(0))
            train_aee_meter.update(aee.item(), ref_img.size(0))

            print(f"ğŸ“Œ Unsupervised Epoch[{epoch + 1}/{total_epochs}] Batch[{batch_idx + 1}/{len(train_loader)}] "
                  f"Total Loss: {total_loss.item():.6f}, AEE: {aee.item():.6f}")

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # è®°å½•è®­ç»ƒé›†æŒ‡æ ‡
        train_writer.add_scalar("TotalLoss", train_loss_meter.avg, epoch)
        train_writer.add_scalar("MSELoss", train_mse_meter.avg, epoch)
        train_writer.add_scalar("CorrLoss", train_corr_meter.avg, epoch)
        train_writer.add_scalar("AEE", train_aee_meter.avg, epoch)
        train_writer.add_scalar("LearningRate", optimizer.param_groups[0]['lr'], epoch)

        # éªŒè¯è¿­ä»£
        val_loss, val_aee = validate_unsupervised(model, val_loader, device, val_writer, epoch)

        # æ‰“å° epoch æ€»ç»“
        print(f"ğŸ“Š Unsupervised Epoch[{epoch + 1}/{total_epochs}] "
              f"Train Loss: {train_loss_meter.avg:.6f}, Train AEE: {train_aee_meter.avg:.6f} | "
              f"Val Loss: {val_loss:.6f}, Val AEE: {val_aee:.6f}")

        # æ¨¡å‹ä¿å­˜ï¼ˆä¿®æ”¹åï¼‰
        # 1. ä¿å­˜å½“å‰epochçš„æ¨¡å‹ï¼ˆæ¯ä¸ªepochéƒ½ä¿å­˜ï¼‰
        save_model(
            model=model,
            epoch=epoch,
            optimizer=optimizer,
            scheduler=scheduler,
            save_dir=os.path.join(save_dir, "model1"),
            model_name=f"dicnet-u1_epoch_{epoch}.pth",  # æ¯ä¸ªepochç”¨ç¼–å·åŒºåˆ†
            is_best=False  # éæœ€ä½³æ¨¡å‹
        )

        # 2. è‹¥å½“å‰éªŒè¯æŸå¤±æ›´ä¼˜ï¼Œä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_model(
                model=model,
                epoch=epoch,
                optimizer=optimizer,
                scheduler=scheduler,
                save_dir=os.path.join(save_dir, "model1"),
                model_name="dicnet-u1_best.pth",  # å›ºå®šåç§°ï¼Œè¦†ç›–æ›´æ–°
                is_best=True  # æ ‡è®°ä¸ºæœ€ä½³æ¨¡å‹
            )
            print(f"ğŸŒŸ New best val loss: {best_val_loss:.6f} (saved as best model)")
    # å…³é—­å†™å…¥å™¨
    train_writer.close()
    val_writer.close()
    print("âœ… Unsupervised training completed!")
    return best_val_loss


# -------------------------- 6. ä¸»å‡½æ•° --------------------------
def main():
    # åŸºç¡€é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» Using device: {device}")

    # è·¯å¾„é…ç½®
    project_root = "/home/dell/DATA/wh/DICNet_Unsupervised/unDICNet_coor"
    save_root = '/home/dell/DATA/wh/DICNet_Unsupervised/unDICNet_coor/result/'
    data_root = '/home/dell/DATA/wh/DATASET/'
    pretrainned=True
    if not pretrainned:
    # æ•°æ®é›†åŠ è½½
        pretrain_dataset = DICDataset(
            root_dir=os.path.join(data_root, "Train1"),
           csv_file=os.path.join(data_root, "Train_annotations_1.csv"),
            is_pretrain=True
        )
        train_dataset = DICDataset(
            root_dir=os.path.join(data_root, "Train1"),
            csv_file=os.path.join(data_root, "Train_annotations_1.csv"),  # æ— ç›‘ç£è®­ç»ƒä»éœ€æ ‡æ³¨è®¡ç®—AEE
            is_pretrain=False
        )
        val_dataset = DICDataset(
            root_dir=os.path.join(data_root, "Train1"),
            csv_file=os.path.join(data_root, "Val_annotations_1.csv"),
            is_pretrain=False
        )

    # æ•°æ®åŠ è½½å™¨
    pretrain_loader = DataLoader(pretrain_dataset, batch_size=4, shuffle=True, num_workers=4)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4)

    print(f"ğŸ“¥ Dataset sizes: "
          f"Pretrain: {len(pretrain_dataset)}, "
          f"Train: {len(train_dataset)}, "
          f"Val: {len(val_dataset)}")

    # æ¨¡å‹ä¸ä¼˜åŒ–å™¨åˆå§‹åŒ–
    model = DICNet().to(device)
    pre_optimizer = optim.Adam(model.parameters(), lr=0.00008, betas=(0.9, 0.999))
    pre_scheduler = StepLR(pre_optimizer, step_size=20, gamma=0.8)
    un_optimizer = optim.Adam(model.parameters(), lr=0.0008, betas=(0.9, 0.999))
    un_scheduler = StepLR(un_optimizer, step_size=20, gamma=0.8)

    # ç›‘ç£é¢„è®­ç»ƒ
    pre_model_load_path = os.path.join(save_root, "model1", "best_pretrain_dicnet.pth")
    pre_start_epoch, _ = load_model(model, pre_optimizer, pre_scheduler, pre_model_load_path)
    pretrain_supervised(
        model=model,
        train_loader=pretrain_loader,
        val_loader=val_loader,
        optimizer=pre_optimizer,
        scheduler=pre_scheduler,
        criterion=nn.MSELoss(),
        device=device,
        save_dir=save_root,
        start_epoch=pre_start_epoch,
        total_epochs=30
    )

    # æ— ç›‘ç£è®­ç»ƒ
    un_model_load_path = os.path.join(save_root, "model1", "best_unsupervised_dicnet.pth")
    un_start_epoch, _ = load_model(model, un_optimizer, un_scheduler, un_model_load_path)
    train_unsupervised(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=un_optimizer,
        scheduler=un_scheduler,
        device=device,
        save_dir=save_root,
        start_epoch=un_start_epoch,
        total_epochs=270
    )


if __name__ == "__main__":
    main()
